import os
import re
import tempfile
import traceback
from typing import List, Dict, Optional, Tuple

import streamlit as st
import pandas as pd
import plotly.express as px

# ----------- OpenAI Client -----------
from openai import OpenAI
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ----------- Dependency Flags ----------
missing_libs = []

try:
    from sentence_transformers import SentenceTransformer
except:
    SentenceTransformer = None
    missing_libs.append("sentence-transformers")

try:
    from sklearn.metrics.pairwise import cosine_similarity
except:
    cosine_similarity = None
    missing_libs.append("scikit-learn")

try:
    from youtube_transcript_api import YouTubeTranscriptApi
except:
    YouTubeTranscriptApi = None
    missing_libs.append("youtube-transcript-api")

try:
    import numpy as np
except:
    np = None
    missing_libs.append("numpy")


# ========= LOGGER =========
def make_logger(container):
    if "logs" not in st.session_state:
        st.session_state["logs"] = []
    def log(msg):
        st.session_state["logs"].append(msg)
        container.code("\n".join(st.session_state["logs"][-200:]))
    return log


# ========= MODEL LOADING =========
@st.cache_resource
def load_embedder(model="all-MiniLM-L6-v2"):
    if SentenceTransformer is None:
        raise RuntimeError("sentence-transformers missing")
    return SentenceTransformer(model)


# ========= VIDEO ID EXTRACTOR =========
def extract_video_id(url):
    if "youtube" in url or "youtu.be" in url:
        from urllib.parse import urlparse, parse_qs
        parsed = urlparse(url)
        vid = parse_qs(parsed.query).get("v", [None])[0]
        if vid: return vid
        return parsed.path.split("/")[-1]
    return url


# ========= 1) YOUTUBE TRANSCRIPT API =========
def fetch_transcript_youtube_api(video_id, log):
    if YouTubeTranscriptApi is None:
        log("youtube-transcript-api missing")
        return None, None

    try:
        log("Trying youtube_transcript_api ...")
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
    except Exception as e:
        log(f"list_transcripts() failed: {e}")
        return None, None

    transcript = None

    # Try manual first
    try:
        langs = [t.language_code for t in transcript_list._manually_created_transcripts]
        if langs:
            transcript = transcript_list.find_transcript(langs)
    except:
        pass

    # Try autogenerated
    if transcript is None:
        try:
            langs = [t.language_code for t in transcript_list._generated_transcripts]
            if langs:
                transcript = transcript_list.find_generated_transcript(langs)
        except:
            transcript = None

    if transcript is None:
        log("No transcript available via API.")
        return None, None

    try:
        fetched = transcript.fetch()
    except Exception as e:
        log(f"Fetch failed: {e}")
        return None, None

    segments = [
        {
            "start": float(s["start"]),
            "end": float(s["start"] + s["duration"]),
            "text": s["text"].strip()
        }
        for s in fetched if s["text"].strip()
    ]

    full_text = " ".join(s["text"] for s in segments)
    log(f"Fetched {len(segments)} segments from YT API")

    return full_text, segments


# ========= 2) OPENAI TRANSCRIPTION =========
def fetch_transcript_openai(url, log):
    try:
        log("Downloading audio via OpenAI (YouTube URL)...")

        # Step1: Convert YouTube â†’ Audio using OpenAI Retrieval
        audio = client.audio.transcriptions.create(
            model="gpt-4o-mini-transcribe",
            input_url=url
        )

        transcript_text = audio.text

        # Split into rough 80-word chunks
        words = transcript_text.split()
        chunks = [" ".join(words[i:i+80]) for i in range(0, len(words), 80)]

        segments = [
            {"start": i, "end": i+1, "text": chunk}
            for i, chunk in enumerate(chunks)
        ]

        log(f"OpenAI transcription success â†’ {len(segments)} segments")

        return transcript_text, segments

    except Exception as e:
        log(f"OpenAI transcription failed: {e}")
        return None, None


# ========= CHUNKING =========
def chunk_manual_text(text, max_words=50):
    words = text.split()
    chunks = [" ".join(words[i:i+max_words]) for i in range(0, len(words), max_words)]
    return [{"start": i, "end": i+1, "text": c} for i, c in enumerate(chunks)]


# ========= SCORING =========
def get_similarity_scores(embedder, title, desc, segments):
    text = title + " " + (desc or "")
    q_emb = embedder.encode([text])
    seg_embs = embedder.encode([s["text"] for s in segments])
    sims = cosine_similarity(q_emb, seg_embs)[0]
    return sims


def compute_score(sims):
    return round(float(sims.mean() * 100), 2)


def build_df(segs, sims):
    df = pd.DataFrame({
        "start": [s["start"] for s in segs],
        "end": [s["end"] for s in segs],
        "text": [s["text"] for s in segs],
        "similarity": sims
    })
    return df.sort_values("start").reset_index(drop=True)


# ========= REASONING =========
def generate_reasoning(title, desc, df, sims, score):
    if df.empty: return "No transcript available."

    high = (df["similarity"] >= 0.60).mean() * 100
    mid = ((df["similarity"] >= 0.40) & (df["similarity"] < 0.60)).mean() * 100
    low = (df["similarity"] < 0.40).mean() * 100

    verdict = "Highly relevant" if score >= 70 else \
              "Moderately relevant" if score >= 40 else \
              "Low relevance"

    return f"""
### ðŸ§  Final Verdict: **{verdict} ({score}%)**

- High-relevance: **{high:.1f}%**
- Medium-relevance: **{mid:.1f}%**
- Low-relevance: **{low:.1f}%**

This score is computed by averaging semantic similarity across all timestamped transcript segments.
"""


# ========= EVALUATION PIPELINE =========
def evaluate_video(title, desc, url, manual_text, chunk_size, log):
    transcript = None
    segments = None

    video_id = extract_video_id(url) if url else None

    # Step 1 â€” Try YouTube API
    if url:
        log("Fetching transcript via YouTube...")
        transcript, segments = fetch_transcript_youtube_api(video_id, log)

    # Step 2 â€” If failed â†’ OpenAI fallback
    if transcript is None and url:
        log("YouTube API failed â†’ switching to OpenAI transcription...")
        transcript, segments = fetch_transcript_openai(url, log)

    # Step 3 â€” Manual fallback
    if manual_text.strip():
        log("Using manual transcript instead.")
        transcript = manual_text.strip()
        segments = chunk_manual_text(transcript, chunk_size)

    if transcript is None or not segments:
        return 0, None, pd.DataFrame(), None, "No transcript available", None

    # Load model
    log("Loading embedding model...")
    embedder = load_embedder()

    # Compute similarity
    log("Computing similarity...")
    sims = get_similarity_scores(embedder, title, desc, segments)

    score = compute_score(sims)
    df = build_df(segments, sims)

    try:
        fig = px.bar(df, x="start", y="similarity", title="Relevance Over Time")
    except:
        fig = None

    reasoning = generate_reasoning(title, desc, df, sims, score)
    return score, fig, df, transcript, None, reasoning


# ========= UI =========
st.set_page_config(page_title="AI Video Relevance Scorer", layout="wide")
st.title("ðŸŽ¯ AI Video Relevance Scorer (with OpenAI Transcription Fallback)")

log_area = st.sidebar.empty()
logger = make_logger(log_area)

chunk_size_words = st.sidebar.number_input("Chunk Size", 20, 300, 80)

title = st.text_input("Video Title")
desc = st.text_input("Description (optional)")
url = st.text_input("YouTube URL")
manual = st.text_area("Or paste transcript")

if st.button("Evaluate"):
    st.session_state["logs"] = []
    logger("Starting evaluation...")

    score, fig, df, transcript, err, reasoning = evaluate_video(
        title, desc, url, manual, chunk_size_words, logger
    )

    if err:
        st.error(err)
        st.stop()

    st.metric("Relevance Score", f"{score} %")
    if fig:
        st.plotly_chart(fig, use_container_width=True)

    st.subheader("Reasoning")
    st.markdown(reasoning)

    st.subheader("Transcript")
    st.write(transcript)

    st.subheader("Segments")
    st.dataframe(df)
